{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面使用梯度下降算法来求解前面的线性回归问题，我们知道，对一元线性回归，损失函数为：\n",
    "\n",
    "$$\n",
    "loss = J(a, b) = \\frac{1}{2n} \\sum_{i=1}^{n}(y_i - (ax_i + b))^2\n",
    "$$\n",
    "\n",
    "针对上面的损失函数，假设选取一个初始点 $(a_0, b_0)$，然后沿着梯度方向对初始值进行更新得到 $(a_1, b_1)$：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a_1 &= a_0 - \\eta \\frac{\\partial J(a, b)}{\\partial a} \\\\\n",
    "b_1 &= b_0 - \\eta \\frac{\\partial J(a, b)}{\\partial b} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "然后再对 $(a_1, b_1)$ 进行更新得到 $(a_2, b_2)$，以此类推，直到损失函数的值不再有显著变化，最终得到 $(a_k, b_k)$ 就是我们要求的参数，它使损失函数达到最小。其中 $\\eta$ 是一个待确定的常数，通常被称为 **学习率**（learning rate），它是一个超参数。\n",
    "\n",
    "上面的迭代过程一般写成下面的形式，其中的 $:=$ 表示使用右侧的值更新左侧的值：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a &:= a - \\eta \\frac{\\partial J(a, b)}{\\partial a} \\\\\n",
    "b &:= b - \\eta \\frac{\\partial J(a, b)}{\\partial b} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "将损失函数带入上式求导，得到：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a &:= a - \\eta \\frac{1}{n} \\sum_{i=1}^{n}(y_i - (ax_i + b))x_i \\\\\n",
    "b &:= b - \\eta \\frac{1}{n} \\sum_{i=1}^{n}(y_i - (ax_i + b))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "我们使用代码来实现这个过程，假设初始值 $(a_0, b_0) = (0, 0)$，学习率 $\\eta = 0.1$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = -148.76089999999996, b = -3.1933333333333334\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([39.93, 42.05, 43.18, 44.68, 49.87, 53.57])\n",
    "Y = np.array([199,   290,   298,   310,   399,   420])\n",
    "\n",
    "eta = 0.01\n",
    "a = 0\n",
    "b = 0\n",
    "\n",
    "def gradient_update_a(X, Y, a, b):\n",
    "    s = 0\n",
    "    for i in range(X.size):\n",
    "        s += (Y[i] - (a*X[i] + b))*X[i]\n",
    "    return a - eta * s / X.size\n",
    "\n",
    "def gradient_update_b(X, Y, a, b):\n",
    "    s = 0\n",
    "    for i in range(X.size):\n",
    "        s += (Y[i] - (a*X[i] + b))\n",
    "    return a - eta * s / X.size\n",
    "\n",
    "a_new = gradient_update_a(X, Y, a, b)\n",
    "b_new = gradient_update_b(X, Y, a, b)\n",
    "print(\"a = {0}, b = {1}\".format(a_new, b_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面是第一次迭代，我们再定义损失函数，然后计算之前的损失函数值和现在的损失函数值，判断是否已经收敛（两次损失函数的差值足够小，就认为收敛）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff = 25435393.956912696\n"
     ]
    }
   ],
   "source": [
    "def loss(X, Y, a, b):\n",
    "    s = 0\n",
    "    for i in range(X.size):\n",
    "        s += (Y[i] - (a*X[i] + b))**2\n",
    "    return s / 2 / X.size\n",
    "\n",
    "diff = abs(loss(X, Y, a, b) - loss(X, Y, a_new, b_new))\n",
    "print(\"diff = {0}\".format(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很显然，这个差值还比较大，我们继续这个过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = -148.76089999999996, b = -3.1933333333333334, diff= 25435393.956912696\n",
      "a = -3418.042080231555, b = -219.74179791999995, diff= 12307585216.628548\n",
      "a = -75332.8851928837, b = -4980.237064353554, diff= 5955314011538.014\n",
      "a = -1657252.5730300485, b = -109697.49900604598, diff= 2881618312409026.0\n",
      "a = -36454935.393463075, b = -2413176.0466161952, diff= 1.394338578743231e+18\n",
      "a = -801903863.6675248, b = -53083078.25447188, diff= 6.746834108975588e+20\n",
      "a = -17639578510.551357, b = -1167675177.4151714, diff= 3.2646138597698585e+23\n",
      "a = -388019988022.0889, b = -25685495291.124634, diff= 1.579659953283901e+26\n",
      "a = -8535323616752.238, b = -565007013522.6543, diff= 7.643555027315778e+28\n",
      "a = -187752568141807.84, b = -12428529083534.078, diff= 3.698513299279935e+31\n",
      "a = -4130016438349338.0, b = -273391889802301.8, diff= 1.7896123696455236e+34\n",
      "a = -9.084848186019032e+16, b = -6013835177700876.0, diff= 8.659459016172268e+36\n",
      "a = -1.9984052798588884e+18, b = -1.3228707541655536e+17, diff= 4.190082261647534e+39\n",
      "a = -4.39591678451358e+19, b = -2.909935142079449e+18, diff= 2.0274695366748127e+42\n",
      "a = -9.669752462690014e+20, b = -6.401020284442111e+19, diff= 9.810386683263068e+44\n",
      "a = -2.1270673962506794e+22, b = -1.4080403404647668e+21, diff= 4.7469855962912064e+47\n",
      "a = -4.678936431568181e+23, b = -3.097283733336787e+22, diff= 2.2969402714614608e+50\n",
      "a = -1.0292314277039444e+25, b = -6.813133295333106e+23, diff= 1.1114284009590227e+53\n",
      "a = -2.2640130877316947e+26, b = -1.4986933486383675e+25, diff= 5.377906886853309e+55\n",
      "a = -4.980177561090578e+27, b = -3.296694275439196e+26, diff= 2.6022263295330865e+58\n",
      "a = -1.0954958111500708e+29, b = -7.251779128524072e+27, diff= 1.259148217435462e+61\n",
      "a = -2.4097756706982696e+30, b = -1.595182814514742e+29, diff= 6.092683851044494e+63\n",
      "a = -5.300813315746943e+31, b = -3.5089433456601564e+30, diff= 2.9480879212443463e+66\n",
      "a = -1.1660264542490832e+33, b = -7.7186660306381445e+31, diff= 1.4265014571364695e+69\n",
      "a = -2.564922797733171e+34, b = -1.6978845032141295e+33, diff= 6.90246173646533e+71\n",
      "a = -5.642092367936968e+35, b = -3.7348575191772537e+34, diff= 3.339917935941514e+74\n",
      "a = -1.2410980290114835e+37, b = -8.215612229305836e+35, diff= 1.616097596005826e+77\n",
      "a = -2.73005866825149e+38, b = -1.8071983725148444e+37, diff= 7.819867104248354e+79\n",
      "a = -6.005343782578947e+39, b = -3.97531658805695e+38, diff= 3.7838260312519586e+82\n",
      "a = -1.3210028914894158e+41, b = -8.744553014005628e+39, diff= 1.8308929351243914e+85\n",
      "a = -2.9058263814732015e+42, b = -1.923550130434529e+41, diff= 8.859204710263258e+87\n",
      "a = -6.391982192972583e+43, b = -4.2312569874852964e+42, diff= 4.286733898671054e+90\n",
      "a = -1.4060522203176024e+45, b = -9.307548272785981e+43, diff= 2.0742366971977862e+93\n",
      "a = -3.092910440885734e+46, b = -2.047392893092205e+45, diff= 1.0036680553779647e+96\n",
      "a = -6.803513309896253e+47, b = -4.503675442587582e+46, diff= 4.856483190886924e+98\n",
      "a = -1.4965772285562775e+49, b = -9.906790514219587e+47, diff= 2.3499232497227732e+101\n",
      "a = -3.2920394199498415e+50, b = -2.1792089493074651e+49, diff= 1.1370654571501045e+104\n",
      "a = -7.241539785393142e+51, b = -4.79363285003897e+50, diff= 5.501957793713074e+106\n",
      "a = -1.5929304535554662e+53, b = -1.0544613405830244e+52, diff= 2.6622512691284643e+109\n",
      "a = -3.503988799429992e+54, b = -2.319511638808779e+53, diff= 1.2881926917132833e+112\n",
      "a = -7.707767454081958e+55, b = -5.102258409582515e+54, diff= 6.23322234916902e+114\n",
      "a = -1.6954871298638137e+57, b = -1.122350086224407e+56, diff= 3.0160907684164853e+117\n",
      "a = -3.7295839873988837e+58, b = -2.4688473513653432e+57, diff= 1.4594062290333478e+120\n",
      "a = -8.20401197629818e+59, b = -5.430754021544194e+58, diff= 7.061679189647082e+122\n",
      "a = -1.8046466505285737e+61, b = -1.1946096718457669e+60, diff= 3.416959033436821e+125\n",
      "a = -3.969703533677084e+62, b = -2.6277976546278336e+61, diff= 1.6533757372188142e+128\n",
      "a = -8.73220590893664e+63, b = -5.780398967470501e+62, diff= 8.000246130180597e+130\n",
      "a = -1.920834122477586e+65, b = -1.271521502589445e+64, diff= 3.871106651845046e+133\n",
      "a = -4.22528255122598e+66, b = -2.796981558964633e+65, diff= 1.8731257096487224e+136\n",
      "a = -9.294406231531873e+67, b = -6.152554892116672e+66, diff= 9.063557891061359e+138\n",
      "a = -2.044502022040474e+69, b = -1.3533851011345706e+68, diff= 4.385614976158023e+141\n",
      "a = -4.497316357818218e+70, b = -2.977057928113643e+69, diff= 2.1220826247571145e+144\n",
      "a = -9.892802356885575e+71, b = -6.548671106187269e+70, diff= 1.0268194291513163e+147\n",
      "a = -2.176131957056257e+73, b = -1.440519274147458e+72, diff= 4.968506540612724e+149\n",
      "a = -4.786864352167578e+74, b = -3.168728045037628e+73, diff= 2.40412837381884e+152\n",
      "a = -1.0529724657437376e+76, b = -6.970290230479875e+74, diff= 1.1632938772559291e+155\n",
      "a = -2.316236542429669e+77, b = -1.5332633538308665e+76, diff= 5.628870153516623e+157\n",
      "a = -5.0950541396133895e+78, b = -3.3727383429754666e+77, diff= 2.7236607898160233e+160\n",
      "a = -1.1207653540583833e+80, b = -7.419054203412276e+78, diff= 1.3179071280133646e+163\n",
      "a = -2.465361396440287e+81, b = -1.631978519407183e+80, diff= 6.3770026156074915e+165\n",
      "a = -5.423085923426381e+82, b = -3.58988331232503e+81, diff= 3.0856622211889583e+168\n",
      "a = -1.1929229108288138e+84, b = -7.896710675328642e+82, diff= 1.4930700075251183e+171\n",
      "a = -2.624087265578816e+85, b = -1.7370492036818438e+84, diff= 7.224569274183501e+173\n",
      "a = -5.772237178837313e+86, b = -3.821008594678129e+85, diff= 3.495777219716087e+176\n",
      "a = -1.2697261438598677e+88, b = -8.405119814486425e+86, diff= 1.6915137645028742e+179\n",
      "a = -2.793032286185426e+89, b = -1.848884590164691e+88, diff= 8.184785916463699e+181\n",
      "a = -6.143867627990767e+90, b = -4.0670142760568464e+89, diff= 3.960400553880842e+184\n",
      "a = -1.3514741529116323e+92, b = -8.946261551229019e+90, diff= 1.9163326575994806e+187\n",
      "a = -2.9728543916977746e+93, b = -1.9679202065795734e+92, diff= 9.2726248383733e+189\n",
      "a = -6.5394245351983574e+94, b = -4.328858392176301e+93, diff= 4.4867769200846045e+192\n",
      "a = -1.4384852945028364e+96, b = -9.522243288555545e+94, diff= 2.171032203017016e+195\n",
      "a = -3.1642538748834234e+97, b = -2.0946196209679172e+96, diff= 1.0505048301906766e+198\n",
      "a = -6.960448343113052e+98, b = -4.6075606593846285e+97, diff= 5.083113906465125e+200\n",
      "a = -1.5310984217070803e+100, b = -1.013530810911566e+99, diff= 2.459583834698724e+203\n",
      "a = -3.367976115034917e+101, b = -2.22947624699151e+100, diff= 1.1901273021281214e+206\n",
      "a = -7.408578671773859e+102, b = -4.9042064458098116e+101, diff= 5.758709970722577e+208\n",
      "a = -1.6296742037701028e+104, b = -1.0787843510590269e+103, diff= 2.786486829400503e+211\n",
      "a = -3.5848144807481987e+105, b = -2.3730152654649847e+104, diff= 1.348306980191296e+214\n",
      "a = -7.885560703883391e+106, b = -5.219950998178443e+105, diff= 6.524099427463258e+216\n",
      "a = -1.7345965306871805e+108, b = -1.1482390703476991e+107, diff= 3.1568384622163337e+219\n",
      "a = -3.815613419588858e+109, b = -2.525795669601183e+108, diff= 1.527510300437479e+222\n",
      "a = -8.393251981184868e+110, b = -5.556023940767198e+109, diff= 7.3912167057938596e+224\n",
      "a = -1.846274008210573e+112, b = -1.2221654507489304e+111, diff= 3.576413486466193e+227\n",
      "a = -4.0612717466784885e+113, b = -2.6884124419342972e+112, diff= 1.7305315126467357e+230\n",
      "a = -8.933629638406174e+114, b = -5.9137340640075854e+113, diff= 8.373582438373086e+232\n",
      "a = -1.965141549109134e+116, b = -1.300851388510958e+115, diff= 4.051754177246444e+235\n",
      "a = -4.322746145008109e+117, b = -2.8614988713985518e+116, diff= 1.9605362500047987e+238\n",
      "a = -9.508798102942521e+118, b = -6.294474421392534e+117, diff= 9.486514283536925e+240\n",
      "a = -2.09166206687702e+120, b = -1.3846033153317467e+119, diff= 4.59027234265777e+243\n",
      "a = -4.601054891110135e+121, b = -3.0457290195859383e+120, diff= 2.2211108896273066e+246\n",
      "a = -1.012099729026313e+123, b = -6.6997277545340175e+121, diff= 1.0747365767767496e+249\n",
      "a = -2.226328278487423e+124, b = -1.4737473917156178e+123, diff= 5.200364893333271e+251\n",
      "a = -4.897281820598047e+125, b = -3.241820345787544e+124, diff= 2.516318473585443e+254\n",
      "a = -1.0772611326957822e+127, b = -7.131072267498892e+125, diff= 1.217579687268609e+257\n",
      "a = -2.3696646232119083e+128, b = -1.5686307771610378e+127, diff= 5.8915447722988076e+259\n",
      "a = -5.2125805490169796e+129, b = -3.45053650104132e+128, diff= 2.850762062388448e+262\n",
      "a = -1.146617783539421e+131, b = -7.590187772910288e+129, diff= 1.3794080585731064e+265\n",
      "a = -2.5222293049779247e+132, b = -1.6696229821261337e+131, diff= 6.674589286705449e+267\n",
      "a = -5.548178964442784e+133, b = -3.6726903020673294e+132, diff= 3.2296565087699235e+270\n",
      "a = -1.2204397816143372e+135, b = -8.078862233749725e+133, diff= 1.5627450194450046e+273\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([39.93, 42.05, 43.18, 44.68, 49.87, 53.57])\n",
    "Y = np.array([199,   290,   298,   310,   399,   420])\n",
    "\n",
    "eta = 0.01\n",
    "a = 0\n",
    "b = 0\n",
    "\n",
    "for i in range(100):\n",
    "    a_new = gradient_update_a(X, Y, a, b)\n",
    "    b_new = gradient_update_b(X, Y, a, b)\n",
    "    diff = abs(loss(X, Y, a, b) - loss(X, Y, a_new, b_new))\n",
    "    print(\"a = {0}, b = {1}, diff= {2}\".format(a_new, b_new, diff))\n",
    "    a = a_new\n",
    "    b = b_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到数据并没有按我们想的那个方向逼近，diff 值变得越来越大，说明损失函数没法收敛。我们试着将输入数据归一化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = [-0.00958073], b = [-2.59052039e-18], diff= [0.00922493]\n",
      "a = [-0.01925726], b = [-0.00958073], diff= [0.00945625]\n",
      "a = [-0.02903056], b = [-0.01935307], diff= [0.00974087]\n",
      "a = [-0.0389016], b = [-0.02922409], diff= [0.0100322]\n",
      "a = [-0.04887134], b = [-0.03919384], diff= [0.01033033]\n",
      "a = [-0.05894078], b = [-0.04926328], diff= [0.01063542]\n",
      "a = [-0.06911091], b = [-0.05943341], diff= [0.01094761]\n",
      "a = [-0.07938275], b = [-0.06970525], diff= [0.01126706]\n",
      "a = [-0.08975731], b = [-0.0800798], diff= [0.01159393]\n",
      "a = [-0.10023561], b = [-0.0905581], diff= [0.01192837]\n",
      "a = [-0.11081869], b = [-0.10114119], diff= [0.01227055]\n",
      "a = [-0.1215076], b = [-0.1118301], diff= [0.01262063]\n",
      "a = [-0.13230341], b = [-0.12262591], diff= [0.01297878]\n",
      "a = [-0.14320717], b = [-0.13352967], diff= [0.01334518]\n",
      "a = [-0.15421997], b = [-0.14454247], diff= [0.01371999]\n",
      "a = [-0.1653429], b = [-0.15566539], diff= [0.01410341]\n",
      "a = [-0.17657705], b = [-0.16689955], diff= [0.0144956]\n",
      "a = [-0.18792355], b = [-0.17824605], diff= [0.01489677]\n",
      "a = [-0.19938351], b = [-0.18970601], diff= [0.0153071]\n",
      "a = [-0.21095808], b = [-0.20128057], diff= [0.01572678]\n",
      "a = [-0.22264838], b = [-0.21297088], diff= [0.01615603]\n",
      "a = [-0.2344556], b = [-0.22477809], diff= [0.01659503]\n",
      "a = [-0.24638088], b = [-0.23670338], diff= [0.01704399]\n",
      "a = [-0.25842541], b = [-0.24874791], diff= [0.01750314]\n",
      "a = [-0.2705904], b = [-0.26091289], diff= [0.01797268]\n",
      "a = [-0.28287703], b = [-0.27319953], diff= [0.01845283]\n",
      "a = [-0.29528653], b = [-0.28560902], diff= [0.01894383]\n",
      "a = [-0.30782012], b = [-0.29814262], diff= [0.01944589]\n",
      "a = [-0.32047905], b = [-0.31080154], diff= [0.01995926]\n",
      "a = [-0.33326456], b = [-0.32358706], diff= [0.02048418]\n",
      "a = [-0.34617794], b = [-0.33650044], diff= [0.02102088]\n",
      "a = [-0.35922044], b = [-0.34954294], diff= [0.02156961]\n",
      "a = [-0.37239338], b = [-0.36271587], diff= [0.02213064]\n",
      "a = [-0.38569804], b = [-0.37602054], diff= [0.02270423]\n",
      "a = [-0.39913575], b = [-0.38945824], diff= [0.02329063]\n",
      "a = [-0.41270783], b = [-0.40303033], diff= [0.02389011]\n",
      "a = [-0.42641564], b = [-0.41673813], diff= [0.02450296]\n",
      "a = [-0.44026052], b = [-0.43058302], diff= [0.02512945]\n",
      "a = [-0.45424385], b = [-0.44456635], diff= [0.02576988]\n",
      "a = [-0.46836702], b = [-0.45868952], diff= [0.02642453]\n",
      "a = [-0.48263142], b = [-0.47295391], diff= [0.02709371]\n",
      "a = [-0.49703846], b = [-0.48736096], diff= [0.02777771]\n",
      "a = [-0.51158957], b = [-0.50191207], diff= [0.02847686]\n",
      "a = [-0.52628619], b = [-0.51660869], diff= [0.02919148]\n",
      "a = [-0.54112978], b = [-0.53145228], diff= [0.02992187]\n",
      "a = [-0.55612181], b = [-0.54644431], diff= [0.03066839]\n",
      "a = [-0.57126375], b = [-0.56158625], diff= [0.03143136]\n",
      "a = [-0.58655712], b = [-0.57687962], diff= [0.03221113]\n",
      "a = [-0.60200342], b = [-0.59232591], diff= [0.03300806]\n",
      "a = [-0.61760418], b = [-0.60792668], diff= [0.0338225]\n",
      "a = [-0.63336095], b = [-0.62368345], diff= [0.03465481]\n",
      "a = [-0.64927528], b = [-0.63959778], diff= [0.03550539]\n",
      "a = [-0.66534877], b = [-0.65567126], diff= [0.0363746]\n",
      "a = [-0.68158298], b = [-0.67190548], diff= [0.03726283]\n",
      "a = [-0.69797954], b = [-0.68830203], diff= [0.03817049]\n",
      "a = [-0.71454006], b = [-0.70486256], diff= [0.03909798]\n",
      "a = [-0.73126619], b = [-0.72158869], diff= [0.04004572]\n",
      "a = [-0.74815958], b = [-0.73848208], diff= [0.04101413]\n",
      "a = [-0.7652219], b = [-0.7555444], diff= [0.04200363]\n",
      "a = [-0.78245485], b = [-0.77277734], diff= [0.04301467]\n",
      "a = [-0.79986012], b = [-0.79018262], diff= [0.04404771]\n",
      "a = [-0.81743945], b = [-0.80776195], diff= [0.04510319]\n",
      "a = [-0.83519457], b = [-0.82551707], diff= [0.04618159]\n",
      "a = [-0.85312725], b = [-0.84344974], diff= [0.04728339]\n",
      "a = [-0.87123925], b = [-0.86156174], diff= [0.04840906]\n",
      "a = [-0.88953237], b = [-0.87985486], diff= [0.04955911]\n",
      "a = [-0.90800842], b = [-0.89833092], diff= [0.05073405]\n",
      "a = [-0.92666923], b = [-0.91699173], diff= [0.0519344]\n",
      "a = [-0.94551665], b = [-0.93583915], diff= [0.05316068]\n",
      "a = [-0.96455254], b = [-0.95487504], diff= [0.05441343]\n",
      "a = [-0.9837788], b = [-0.97410129], diff= [0.0556932]\n",
      "a = [-1.00319731], b = [-0.99351981], diff= [0.05700055]\n",
      "a = [-1.02281001], b = [-1.01313251], diff= [0.05833607]\n",
      "a = [-1.04261884], b = [-1.03294134], diff= [0.05970032]\n",
      "a = [-1.06262576], b = [-1.05294825], diff= [0.06109392]\n",
      "a = [-1.08283274], b = [-1.07315524], diff= [0.06251746]\n",
      "a = [-1.1032418], b = [-1.09356429], diff= [0.06397157]\n",
      "a = [-1.12385494], b = [-1.11417744], diff= [0.06545688]\n",
      "a = [-1.14467422], b = [-1.13499672], diff= [0.06697404]\n",
      "a = [-1.16570169], b = [-1.15602418], diff= [0.06852371]\n",
      "a = [-1.18693943], b = [-1.17726193], diff= [0.07010657]\n",
      "a = [-1.20838955], b = [-1.19871205], diff= [0.07172329]\n",
      "a = [-1.23005418], b = [-1.22037667], diff= [0.07337459]\n",
      "a = [-1.25193545], b = [-1.24225794], diff= [0.07506117]\n",
      "a = [-1.27403553], b = [-1.26435802], diff= [0.07678378]\n",
      "a = [-1.29635661], b = [-1.28667911], diff= [0.07854314]\n",
      "a = [-1.3189009], b = [-1.3092234], diff= [0.08034003]\n",
      "a = [-1.34167064], b = [-1.33199314], diff= [0.08217522]\n",
      "a = [-1.36466807], b = [-1.35499057], diff= [0.0840495]\n",
      "a = [-1.38789548], b = [-1.37821798], diff= [0.08596368]\n",
      "a = [-1.41135516], b = [-1.40167766], diff= [0.08791858]\n",
      "a = [-1.43504944], b = [-1.42537194], diff= [0.08991505]\n",
      "a = [-1.45898067], b = [-1.44930316], diff= [0.09195393]\n",
      "a = [-1.4831512], b = [-1.4734737], diff= [0.09403612]\n",
      "a = [-1.50756344], b = [-1.49788594], diff= [0.09616249]\n",
      "a = [-1.5322198], b = [-1.5225423], diff= [0.09833397]\n",
      "a = [-1.55712273], b = [-1.54744522], diff= [0.10055148]\n",
      "a = [-1.58227468], b = [-1.57259718], diff= [0.10281598]\n",
      "a = [-1.60767816], b = [-1.59800065], diff= [0.10512842]\n",
      "a = [-1.63333566], b = [-1.62365816], diff= [0.1074898]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aneasystone/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/aneasystone/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = np.array([39.93, 42.05, 43.18, 44.68, 49.87, 53.57]).reshape(-1,1)\n",
    "Y = np.array([199,   290,   298,   310,   399,   420]).reshape(-1,1)\n",
    "\n",
    "x_scalar = StandardScaler()\n",
    "y_scalar = StandardScaler()\n",
    "x_scalar.fit(X)\n",
    "y_scalar.fit(Y)\n",
    "X_standard = x_scalar.transform(X)\n",
    "Y_standard = y_scalar.transform(Y)\n",
    "\n",
    "eta = 0.01\n",
    "a = 0\n",
    "b = 0\n",
    "\n",
    "for i in range(100):\n",
    "    a_new = gradient_update_a(X_standard, Y_standard, a, b)\n",
    "    b_new = gradient_update_b(X_standard, Y_standard, a, b)\n",
    "    diff = abs(loss(X_standard, Y_standard, a, b) - loss(X_standard, Y_standard, a_new, b_new))\n",
    "    print(\"a = {0}, b = {1}, diff= {2}\".format(a_new, b_new, diff))\n",
    "    a = a_new\n",
    "    b = b_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降的一般形式\n",
    "\n",
    "回到线性回归的一般形式，损失函数可以写成下面这样：\n",
    "\n",
    "$$\n",
    "loss = J(\\theta) = (\\bf{y}-\\rm{X}\\theta)^T (\\bf{y}-\\rm{X}\\theta)\n",
    "$$\n",
    "\n",
    "根据前面的矩阵求导，得到梯度下降的更新公式为：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta &:= \\theta - \\eta \\frac{\\partial}{\\partial\\theta}J(\\theta) \\\\\n",
    "       &= \\theta - \\eta X^T(\\bf{y}-\\rm{X}\\theta)\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标准化和归一化\n",
    "\n",
    "在实际的运用中我们选取的特征，比如长度，重量，面积等等，通常单位和范围都不同，这会导致梯度下降算法变慢，所以我们要将特征缩放到相对统一的范围内。通常的方法有 **标准化（Standardization）** 和 **归一化（Normalization）**。\n",
    "\n",
    "标准化是把数据变成符合标准的正态分布，由 **中心极限定理** 可知，当数据量足够大时，无论原来的数据是何种分布，都可以通过下面的更新公式转变成正态分布：\n",
    "\n",
    "$$\n",
    "x_i := \\frac{x_i-\\mu}{\\delta}\n",
    "$$\n",
    "\n",
    "归一化对梯度下降算法很友好，可以让算法最终收敛并且提高训练速度和精度，归一化的更新公式为：\n",
    "\n",
    "$$\n",
    "x_i := \\frac{x_i-min(x_i)}{max(x_i)-min(x_i)}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
