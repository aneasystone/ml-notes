{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面使用梯度下降算法来求解前面的线性回归问题，我们知道，对一元线性回归，损失函数为：\n",
    "\n",
    "$$\n",
    "loss = J(a, b) = \\frac{1}{2n} \\sum_{i=1}^{n}(y_i - (ax_i + b))^2\n",
    "$$\n",
    "\n",
    "针对上面的损失函数，假设选取一个初始点 $(a_0, b_0)$，然后沿着梯度方向对初始值进行更新得到 $(a_1, b_1)$：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a_1 &= a_0 - \\eta \\frac{\\partial J(a, b)}{\\partial a} \\\\\n",
    "b_1 &= b_0 - \\eta \\frac{\\partial J(a, b)}{\\partial b} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "然后再对 $(a_1, b_1)$ 进行更新得到 $(a_2, b_2)$，以此类推，直到损失函数的值不再有显著变化，最终得到 $(a_k, b_k)$ 就是我们要求的参数，它使损失函数达到最小。其中 $\\eta$ 是一个待确定的常数，通常被称为 **学习率**（learning rate），它是一个超参数。\n",
    "\n",
    "上面的迭代过程一般写成下面的形式，其中的 $:=$ 表示使用右侧的值更新左侧的值：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a &:= a - \\eta \\frac{\\partial J(a, b)}{\\partial a} \\\\\n",
    "b &:= b - \\eta \\frac{\\partial J(a, b)}{\\partial b} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "将损失函数带入上式求导，得到：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a &:= a - \\eta \\frac{1}{n} \\sum_{i=1}^{n}(y_i - (ax_i + b))x_i \\\\\n",
    "b &:= b - \\eta \\frac{1}{n} \\sum_{i=1}^{n}(y_i - (ax_i + b))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "我们使用代码来实现这个过程，假设初始值 $(a_0, b_0) = (0, 0)$，学习率 $\\eta = 0.1$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = -148.76089999999996, b = -3.1933333333333334\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([39.93, 42.05, 43.18, 44.68, 49.87, 53.57])\n",
    "Y = np.array([199,   290,   298,   310,   399,   420])\n",
    "\n",
    "eta = 0.01\n",
    "a = 0\n",
    "b = 0\n",
    "\n",
    "def gradient_update_a(X, Y, a, b):\n",
    "    s = 0\n",
    "    for i in range(X.size):\n",
    "        s += (Y[i] - (a*X[i] + b))*X[i]\n",
    "    return a - eta * s / X.size\n",
    "\n",
    "def gradient_update_b(X, Y, a, b):\n",
    "    s = 0\n",
    "    for i in range(X.size):\n",
    "        s += (Y[i] - (a*X[i] + b))\n",
    "    return a - eta * s / X.size\n",
    "\n",
    "a_new = gradient_update_a(X, Y, a, b)\n",
    "b_new = gradient_update_b(X, Y, a, b)\n",
    "print(\"a = {0}, b = {1}\".format(a_new, b_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面是第一次迭代，我们再定义损失函数，然后计算之前的损失函数值和现在的损失函数值，判断是否已经收敛（两次损失函数的差值足够小，就认为收敛）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff = 25435393.956912696\n"
     ]
    }
   ],
   "source": [
    "def loss(X, Y, a, b):\n",
    "    s = 0\n",
    "    for i in range(X.size):\n",
    "        s += (Y[i] - (a*X[i] + b))**2\n",
    "    return s / 2 / X.size\n",
    "\n",
    "diff = abs(loss(X, Y, a, b) - loss(X, Y, a_new, b_new))\n",
    "print(\"diff = {0}\".format(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很显然，这个差值还比较大，我们继续这个过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = -148.76089999999996, b = -3.1933333333333334, diff= 25435393.956912696\n",
      "a = -3418.042080231555, b = -219.74179791999995, diff= 12307585216.628548\n",
      "a = -75332.8851928837, b = -4980.237064353554, diff= 5955314011538.014\n",
      "a = -1657252.5730300485, b = -109697.49900604598, diff= 2881618312409026.0\n",
      "a = -36454935.393463075, b = -2413176.0466161952, diff= 1.394338578743231e+18\n",
      "a = -801903863.6675248, b = -53083078.25447188, diff= 6.746834108975588e+20\n",
      "a = -17639578510.551357, b = -1167675177.4151714, diff= 3.2646138597698585e+23\n",
      "a = -388019988022.0889, b = -25685495291.124634, diff= 1.579659953283901e+26\n",
      "a = -8535323616752.238, b = -565007013522.6543, diff= 7.643555027315778e+28\n",
      "a = -187752568141807.84, b = -12428529083534.078, diff= 3.698513299279935e+31\n",
      "a = -4130016438349338.0, b = -273391889802301.8, diff= 1.7896123696455236e+34\n",
      "a = -9.084848186019032e+16, b = -6013835177700876.0, diff= 8.659459016172268e+36\n",
      "a = -1.9984052798588884e+18, b = -1.3228707541655536e+17, diff= 4.190082261647534e+39\n",
      "a = -4.39591678451358e+19, b = -2.909935142079449e+18, diff= 2.0274695366748127e+42\n",
      "a = -9.669752462690014e+20, b = -6.401020284442111e+19, diff= 9.810386683263068e+44\n",
      "a = -2.1270673962506794e+22, b = -1.4080403404647668e+21, diff= 4.7469855962912064e+47\n",
      "a = -4.678936431568181e+23, b = -3.097283733336787e+22, diff= 2.2969402714614608e+50\n",
      "a = -1.0292314277039444e+25, b = -6.813133295333106e+23, diff= 1.1114284009590227e+53\n",
      "a = -2.2640130877316947e+26, b = -1.4986933486383675e+25, diff= 5.377906886853309e+55\n",
      "a = -4.980177561090578e+27, b = -3.296694275439196e+26, diff= 2.6022263295330865e+58\n",
      "a = -1.0954958111500708e+29, b = -7.251779128524072e+27, diff= 1.259148217435462e+61\n",
      "a = -2.4097756706982696e+30, b = -1.595182814514742e+29, diff= 6.092683851044494e+63\n",
      "a = -5.300813315746943e+31, b = -3.5089433456601564e+30, diff= 2.9480879212443463e+66\n",
      "a = -1.1660264542490832e+33, b = -7.7186660306381445e+31, diff= 1.4265014571364695e+69\n",
      "a = -2.564922797733171e+34, b = -1.6978845032141295e+33, diff= 6.90246173646533e+71\n",
      "a = -5.642092367936968e+35, b = -3.7348575191772537e+34, diff= 3.339917935941514e+74\n",
      "a = -1.2410980290114835e+37, b = -8.215612229305836e+35, diff= 1.616097596005826e+77\n",
      "a = -2.73005866825149e+38, b = -1.8071983725148444e+37, diff= 7.819867104248354e+79\n",
      "a = -6.005343782578947e+39, b = -3.97531658805695e+38, diff= 3.7838260312519586e+82\n",
      "a = -1.3210028914894158e+41, b = -8.744553014005628e+39, diff= 1.8308929351243914e+85\n",
      "a = -2.9058263814732015e+42, b = -1.923550130434529e+41, diff= 8.859204710263258e+87\n",
      "a = -6.391982192972583e+43, b = -4.2312569874852964e+42, diff= 4.286733898671054e+90\n",
      "a = -1.4060522203176024e+45, b = -9.307548272785981e+43, diff= 2.0742366971977862e+93\n",
      "a = -3.092910440885734e+46, b = -2.047392893092205e+45, diff= 1.0036680553779647e+96\n",
      "a = -6.803513309896253e+47, b = -4.503675442587582e+46, diff= 4.856483190886924e+98\n",
      "a = -1.4965772285562775e+49, b = -9.906790514219587e+47, diff= 2.3499232497227732e+101\n",
      "a = -3.2920394199498415e+50, b = -2.1792089493074651e+49, diff= 1.1370654571501045e+104\n",
      "a = -7.241539785393142e+51, b = -4.79363285003897e+50, diff= 5.501957793713074e+106\n",
      "a = -1.5929304535554662e+53, b = -1.0544613405830244e+52, diff= 2.6622512691284643e+109\n",
      "a = -3.503988799429992e+54, b = -2.319511638808779e+53, diff= 1.2881926917132833e+112\n",
      "a = -7.707767454081958e+55, b = -5.102258409582515e+54, diff= 6.23322234916902e+114\n",
      "a = -1.6954871298638137e+57, b = -1.122350086224407e+56, diff= 3.0160907684164853e+117\n",
      "a = -3.7295839873988837e+58, b = -2.4688473513653432e+57, diff= 1.4594062290333478e+120\n",
      "a = -8.20401197629818e+59, b = -5.430754021544194e+58, diff= 7.061679189647082e+122\n",
      "a = -1.8046466505285737e+61, b = -1.1946096718457669e+60, diff= 3.416959033436821e+125\n",
      "a = -3.969703533677084e+62, b = -2.6277976546278336e+61, diff= 1.6533757372188142e+128\n",
      "a = -8.73220590893664e+63, b = -5.780398967470501e+62, diff= 8.000246130180597e+130\n",
      "a = -1.920834122477586e+65, b = -1.271521502589445e+64, diff= 3.871106651845046e+133\n",
      "a = -4.22528255122598e+66, b = -2.796981558964633e+65, diff= 1.8731257096487224e+136\n",
      "a = -9.294406231531873e+67, b = -6.152554892116672e+66, diff= 9.063557891061359e+138\n",
      "a = -2.044502022040474e+69, b = -1.3533851011345706e+68, diff= 4.385614976158023e+141\n",
      "a = -4.497316357818218e+70, b = -2.977057928113643e+69, diff= 2.1220826247571145e+144\n",
      "a = -9.892802356885575e+71, b = -6.548671106187269e+70, diff= 1.0268194291513163e+147\n",
      "a = -2.176131957056257e+73, b = -1.440519274147458e+72, diff= 4.968506540612724e+149\n",
      "a = -4.786864352167578e+74, b = -3.168728045037628e+73, diff= 2.40412837381884e+152\n",
      "a = -1.0529724657437376e+76, b = -6.970290230479875e+74, diff= 1.1632938772559291e+155\n",
      "a = -2.316236542429669e+77, b = -1.5332633538308665e+76, diff= 5.628870153516623e+157\n",
      "a = -5.0950541396133895e+78, b = -3.3727383429754666e+77, diff= 2.7236607898160233e+160\n",
      "a = -1.1207653540583833e+80, b = -7.419054203412276e+78, diff= 1.3179071280133646e+163\n",
      "a = -2.465361396440287e+81, b = -1.631978519407183e+80, diff= 6.3770026156074915e+165\n",
      "a = -5.423085923426381e+82, b = -3.58988331232503e+81, diff= 3.0856622211889583e+168\n",
      "a = -1.1929229108288138e+84, b = -7.896710675328642e+82, diff= 1.4930700075251183e+171\n",
      "a = -2.624087265578816e+85, b = -1.7370492036818438e+84, diff= 7.224569274183501e+173\n",
      "a = -5.772237178837313e+86, b = -3.821008594678129e+85, diff= 3.495777219716087e+176\n",
      "a = -1.2697261438598677e+88, b = -8.405119814486425e+86, diff= 1.6915137645028742e+179\n",
      "a = -2.793032286185426e+89, b = -1.848884590164691e+88, diff= 8.184785916463699e+181\n",
      "a = -6.143867627990767e+90, b = -4.0670142760568464e+89, diff= 3.960400553880842e+184\n",
      "a = -1.3514741529116323e+92, b = -8.946261551229019e+90, diff= 1.9163326575994806e+187\n",
      "a = -2.9728543916977746e+93, b = -1.9679202065795734e+92, diff= 9.2726248383733e+189\n",
      "a = -6.5394245351983574e+94, b = -4.328858392176301e+93, diff= 4.4867769200846045e+192\n",
      "a = -1.4384852945028364e+96, b = -9.522243288555545e+94, diff= 2.171032203017016e+195\n",
      "a = -3.1642538748834234e+97, b = -2.0946196209679172e+96, diff= 1.0505048301906766e+198\n",
      "a = -6.960448343113052e+98, b = -4.6075606593846285e+97, diff= 5.083113906465125e+200\n",
      "a = -1.5310984217070803e+100, b = -1.013530810911566e+99, diff= 2.459583834698724e+203\n",
      "a = -3.367976115034917e+101, b = -2.22947624699151e+100, diff= 1.1901273021281214e+206\n",
      "a = -7.408578671773859e+102, b = -4.9042064458098116e+101, diff= 5.758709970722577e+208\n",
      "a = -1.6296742037701028e+104, b = -1.0787843510590269e+103, diff= 2.786486829400503e+211\n",
      "a = -3.5848144807481987e+105, b = -2.3730152654649847e+104, diff= 1.348306980191296e+214\n",
      "a = -7.885560703883391e+106, b = -5.219950998178443e+105, diff= 6.524099427463258e+216\n",
      "a = -1.7345965306871805e+108, b = -1.1482390703476991e+107, diff= 3.1568384622163337e+219\n",
      "a = -3.815613419588858e+109, b = -2.525795669601183e+108, diff= 1.527510300437479e+222\n",
      "a = -8.393251981184868e+110, b = -5.556023940767198e+109, diff= 7.3912167057938596e+224\n",
      "a = -1.846274008210573e+112, b = -1.2221654507489304e+111, diff= 3.576413486466193e+227\n",
      "a = -4.0612717466784885e+113, b = -2.6884124419342972e+112, diff= 1.7305315126467357e+230\n",
      "a = -8.933629638406174e+114, b = -5.9137340640075854e+113, diff= 8.373582438373086e+232\n",
      "a = -1.965141549109134e+116, b = -1.300851388510958e+115, diff= 4.051754177246444e+235\n",
      "a = -4.322746145008109e+117, b = -2.8614988713985518e+116, diff= 1.9605362500047987e+238\n",
      "a = -9.508798102942521e+118, b = -6.294474421392534e+117, diff= 9.486514283536925e+240\n",
      "a = -2.09166206687702e+120, b = -1.3846033153317467e+119, diff= 4.59027234265777e+243\n",
      "a = -4.601054891110135e+121, b = -3.0457290195859383e+120, diff= 2.2211108896273066e+246\n",
      "a = -1.012099729026313e+123, b = -6.6997277545340175e+121, diff= 1.0747365767767496e+249\n",
      "a = -2.226328278487423e+124, b = -1.4737473917156178e+123, diff= 5.200364893333271e+251\n",
      "a = -4.897281820598047e+125, b = -3.241820345787544e+124, diff= 2.516318473585443e+254\n",
      "a = -1.0772611326957822e+127, b = -7.131072267498892e+125, diff= 1.217579687268609e+257\n",
      "a = -2.3696646232119083e+128, b = -1.5686307771610378e+127, diff= 5.8915447722988076e+259\n",
      "a = -5.2125805490169796e+129, b = -3.45053650104132e+128, diff= 2.850762062388448e+262\n",
      "a = -1.146617783539421e+131, b = -7.590187772910288e+129, diff= 1.3794080585731064e+265\n",
      "a = -2.5222293049779247e+132, b = -1.6696229821261337e+131, diff= 6.674589286705449e+267\n",
      "a = -5.548178964442784e+133, b = -3.6726903020673294e+132, diff= 3.2296565087699235e+270\n",
      "a = -1.2204397816143372e+135, b = -8.078862233749725e+133, diff= 1.5627450194450046e+273\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([39.93, 42.05, 43.18, 44.68, 49.87, 53.57])\n",
    "Y = np.array([199,   290,   298,   310,   399,   420])\n",
    "\n",
    "eta = 0.01\n",
    "a = 0\n",
    "b = 0\n",
    "\n",
    "for i in range(100):\n",
    "    a_new = gradient_update_a(X, Y, a, b)\n",
    "    b_new = gradient_update_b(X, Y, a, b)\n",
    "    diff = abs(loss(X, Y, a, b) - loss(X, Y, a_new, b_new))\n",
    "    print(\"a = {0}, b = {1}, diff= {2}\".format(a_new, b_new, diff))\n",
    "    a = a_new\n",
    "    b = b_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到数据并没有按我们想的那个方向逼近，diff 值变得越来越大，说明损失函数没法收敛。我们试着将输入数据归一化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = [-0.00958073], b = [-2.59052039e-18], diff= [0.00922493]\n",
      "a = [-0.01925726], b = [-0.00958073], diff= [0.00945625]\n",
      "a = [-0.02903056], b = [-0.01935307], diff= [0.00974087]\n",
      "a = [-0.0389016], b = [-0.02922409], diff= [0.0100322]\n",
      "a = [-0.04887134], b = [-0.03919384], diff= [0.01033033]\n",
      "a = [-0.05894078], b = [-0.04926328], diff= [0.01063542]\n",
      "a = [-0.06911091], b = [-0.05943341], diff= [0.01094761]\n",
      "a = [-0.07938275], b = [-0.06970525], diff= [0.01126706]\n",
      "a = [-0.08975731], b = [-0.0800798], diff= [0.01159393]\n",
      "a = [-0.10023561], b = [-0.0905581], diff= [0.01192837]\n",
      "a = [-0.11081869], b = [-0.10114119], diff= [0.01227055]\n",
      "a = [-0.1215076], b = [-0.1118301], diff= [0.01262063]\n",
      "a = [-0.13230341], b = [-0.12262591], diff= [0.01297878]\n",
      "a = [-0.14320717], b = [-0.13352967], diff= [0.01334518]\n",
      "a = [-0.15421997], b = [-0.14454247], diff= [0.01371999]\n",
      "a = [-0.1653429], b = [-0.15566539], diff= [0.01410341]\n",
      "a = [-0.17657705], b = [-0.16689955], diff= [0.0144956]\n",
      "a = [-0.18792355], b = [-0.17824605], diff= [0.01489677]\n",
      "a = [-0.19938351], b = [-0.18970601], diff= [0.0153071]\n",
      "a = [-0.21095808], b = [-0.20128057], diff= [0.01572678]\n",
      "a = [-0.22264838], b = [-0.21297088], diff= [0.01615603]\n",
      "a = [-0.2344556], b = [-0.22477809], diff= [0.01659503]\n",
      "a = [-0.24638088], b = [-0.23670338], diff= [0.01704399]\n",
      "a = [-0.25842541], b = [-0.24874791], diff= [0.01750314]\n",
      "a = [-0.2705904], b = [-0.26091289], diff= [0.01797268]\n",
      "a = [-0.28287703], b = [-0.27319953], diff= [0.01845283]\n",
      "a = [-0.29528653], b = [-0.28560902], diff= [0.01894383]\n",
      "a = [-0.30782012], b = [-0.29814262], diff= [0.01944589]\n",
      "a = [-0.32047905], b = [-0.31080154], diff= [0.01995926]\n",
      "a = [-0.33326456], b = [-0.32358706], diff= [0.02048418]\n",
      "a = [-0.34617794], b = [-0.33650044], diff= [0.02102088]\n",
      "a = [-0.35922044], b = [-0.34954294], diff= [0.02156961]\n",
      "a = [-0.37239338], b = [-0.36271587], diff= [0.02213064]\n",
      "a = [-0.38569804], b = [-0.37602054], diff= [0.02270423]\n",
      "a = [-0.39913575], b = [-0.38945824], diff= [0.02329063]\n",
      "a = [-0.41270783], b = [-0.40303033], diff= [0.02389011]\n",
      "a = [-0.42641564], b = [-0.41673813], diff= [0.02450296]\n",
      "a = [-0.44026052], b = [-0.43058302], diff= [0.02512945]\n",
      "a = [-0.45424385], b = [-0.44456635], diff= [0.02576988]\n",
      "a = [-0.46836702], b = [-0.45868952], diff= [0.02642453]\n",
      "a = [-0.48263142], b = [-0.47295391], diff= [0.02709371]\n",
      "a = [-0.49703846], b = [-0.48736096], diff= [0.02777771]\n",
      "a = [-0.51158957], b = [-0.50191207], diff= [0.02847686]\n",
      "a = [-0.52628619], b = [-0.51660869], diff= [0.02919148]\n",
      "a = [-0.54112978], b = [-0.53145228], diff= [0.02992187]\n",
      "a = [-0.55612181], b = [-0.54644431], diff= [0.03066839]\n",
      "a = [-0.57126375], b = [-0.56158625], diff= [0.03143136]\n",
      "a = [-0.58655712], b = [-0.57687962], diff= [0.03221113]\n",
      "a = [-0.60200342], b = [-0.59232591], diff= [0.03300806]\n",
      "a = [-0.61760418], b = [-0.60792668], diff= [0.0338225]\n",
      "a = [-0.63336095], b = [-0.62368345], diff= [0.03465481]\n",
      "a = [-0.64927528], b = [-0.63959778], diff= [0.03550539]\n",
      "a = [-0.66534877], b = [-0.65567126], diff= [0.0363746]\n",
      "a = [-0.68158298], b = [-0.67190548], diff= [0.03726283]\n",
      "a = [-0.69797954], b = [-0.68830203], diff= [0.03817049]\n",
      "a = [-0.71454006], b = [-0.70486256], diff= [0.03909798]\n",
      "a = [-0.73126619], b = [-0.72158869], diff= [0.04004572]\n",
      "a = [-0.74815958], b = [-0.73848208], diff= [0.04101413]\n",
      "a = [-0.7652219], b = [-0.7555444], diff= [0.04200363]\n",
      "a = [-0.78245485], b = [-0.77277734], diff= [0.04301467]\n",
      "a = [-0.79986012], b = [-0.79018262], diff= [0.04404771]\n",
      "a = [-0.81743945], b = [-0.80776195], diff= [0.04510319]\n",
      "a = [-0.83519457], b = [-0.82551707], diff= [0.04618159]\n",
      "a = [-0.85312725], b = [-0.84344974], diff= [0.04728339]\n",
      "a = [-0.87123925], b = [-0.86156174], diff= [0.04840906]\n",
      "a = [-0.88953237], b = [-0.87985486], diff= [0.04955911]\n",
      "a = [-0.90800842], b = [-0.89833092], diff= [0.05073405]\n",
      "a = [-0.92666923], b = [-0.91699173], diff= [0.0519344]\n",
      "a = [-0.94551665], b = [-0.93583915], diff= [0.05316068]\n",
      "a = [-0.96455254], b = [-0.95487504], diff= [0.05441343]\n",
      "a = [-0.9837788], b = [-0.97410129], diff= [0.0556932]\n",
      "a = [-1.00319731], b = [-0.99351981], diff= [0.05700055]\n",
      "a = [-1.02281001], b = [-1.01313251], diff= [0.05833607]\n",
      "a = [-1.04261884], b = [-1.03294134], diff= [0.05970032]\n",
      "a = [-1.06262576], b = [-1.05294825], diff= [0.06109392]\n",
      "a = [-1.08283274], b = [-1.07315524], diff= [0.06251746]\n",
      "a = [-1.1032418], b = [-1.09356429], diff= [0.06397157]\n",
      "a = [-1.12385494], b = [-1.11417744], diff= [0.06545688]\n",
      "a = [-1.14467422], b = [-1.13499672], diff= [0.06697404]\n",
      "a = [-1.16570169], b = [-1.15602418], diff= [0.06852371]\n",
      "a = [-1.18693943], b = [-1.17726193], diff= [0.07010657]\n",
      "a = [-1.20838955], b = [-1.19871205], diff= [0.07172329]\n",
      "a = [-1.23005418], b = [-1.22037667], diff= [0.07337459]\n",
      "a = [-1.25193545], b = [-1.24225794], diff= [0.07506117]\n",
      "a = [-1.27403553], b = [-1.26435802], diff= [0.07678378]\n",
      "a = [-1.29635661], b = [-1.28667911], diff= [0.07854314]\n",
      "a = [-1.3189009], b = [-1.3092234], diff= [0.08034003]\n",
      "a = [-1.34167064], b = [-1.33199314], diff= [0.08217522]\n",
      "a = [-1.36466807], b = [-1.35499057], diff= [0.0840495]\n",
      "a = [-1.38789548], b = [-1.37821798], diff= [0.08596368]\n",
      "a = [-1.41135516], b = [-1.40167766], diff= [0.08791858]\n",
      "a = [-1.43504944], b = [-1.42537194], diff= [0.08991505]\n",
      "a = [-1.45898067], b = [-1.44930316], diff= [0.09195393]\n",
      "a = [-1.4831512], b = [-1.4734737], diff= [0.09403612]\n",
      "a = [-1.50756344], b = [-1.49788594], diff= [0.09616249]\n",
      "a = [-1.5322198], b = [-1.5225423], diff= [0.09833397]\n",
      "a = [-1.55712273], b = [-1.54744522], diff= [0.10055148]\n",
      "a = [-1.58227468], b = [-1.57259718], diff= [0.10281598]\n",
      "a = [-1.60767816], b = [-1.59800065], diff= [0.10512842]\n",
      "a = [-1.63333566], b = [-1.62365816], diff= [0.1074898]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aneasystone/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/aneasystone/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = np.array([39.93, 42.05, 43.18, 44.68, 49.87, 53.57]).reshape(-1,1)\n",
    "Y = np.array([199,   290,   298,   310,   399,   420]).reshape(-1,1)\n",
    "\n",
    "x_scalar = StandardScaler()\n",
    "y_scalar = StandardScaler()\n",
    "x_scalar.fit(X)\n",
    "y_scalar.fit(Y)\n",
    "X_standard = x_scalar.transform(X)\n",
    "Y_standard = y_scalar.transform(Y)\n",
    "\n",
    "eta = 0.01\n",
    "a = 0\n",
    "b = 0\n",
    "\n",
    "for i in range(100):\n",
    "    a_new = gradient_update_a(X_standard, Y_standard, a, b)\n",
    "    b_new = gradient_update_b(X_standard, Y_standard, a, b)\n",
    "    diff = abs(loss(X_standard, Y_standard, a, b) - loss(X_standard, Y_standard, a_new, b_new))\n",
    "    print(\"a = {0}, b = {1}, diff= {2}\".format(a_new, b_new, diff))\n",
    "    a = a_new\n",
    "    b = b_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降的一般形式\n",
    "\n",
    "回到线性回归的一般形式，损失函数可以写成下面这样：\n",
    "\n",
    "$$\n",
    "loss = J(\\theta) = (\\bf{y}-\\rm{X}\\theta)^T (\\bf{y}-\\rm{X}\\theta)\n",
    "$$\n",
    "\n",
    "根据前面的矩阵求导，得到梯度下降的更新公式为：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta &:= \\theta - \\eta \\frac{\\partial}{\\partial\\theta}J(\\theta) \\\\\n",
    "       &= \\theta - \\eta X^T(\\bf{y}-\\rm{X}\\theta)\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标准化和归一化\n",
    "\n",
    "在实际的运用中我们选取的特征，比如长度，重量，面积等等，通常单位和范围都不同，这会导致梯度下降算法变慢，所以我们要将特征缩放到相对统一的范围内。通常的方法有 **标准化（Standardization）** 和 **归一化（Normalization）**。\n",
    "\n",
    "标准化是把数据变成符合标准的正态分布，由 **中心极限定理** 可知，当数据量足够大时，无论原来的数据是何种分布，都可以通过下面的更新公式转变成正态分布：\n",
    "\n",
    "$$\n",
    "x_i := \\frac{x_i-\\mu}{\\delta}\n",
    "$$\n",
    "\n",
    "归一化对梯度下降算法很友好，可以让算法最终收敛并且提高训练速度和精度，归一化的更新公式为：\n",
    "\n",
    "$$\n",
    "x_i := \\frac{x_i-min(x_i)}{max(x_i)-min(x_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SGDRegressor(max_iter=100000, tol=0.01)\n",
    "model.fit(X.reshape(-1,1), Y.ravel())\n",
    "\n",
    "plt.plot(X, Y, 'k.')\n",
    "\n",
    "x = [[0],[10]]\n",
    "y = model.predict(x)\n",
    "\n",
    "plt.plot(x, y, 'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 1.997, b = 1.999, loss = 2.808066000000002, diff= 2.778066000000002\n",
      "a = 2.02382, b = 2.02588, loss = 3.1172770548000006, diff= 0.30921105479999866\n",
      "a = 2.0527874, b = 2.054311, loss = 3.470038981045202, diff= 0.35276192624520153\n",
      "a = 2.0840561, b = 2.0850003520000002, loss = 3.8767760893829095, diff= 0.4067371083377074\n",
      "a = 2.1178089155600004, b = 2.1181277935600002, loss = 4.346065619973978, diff= 0.4692895305910687\n",
      "a = 2.1542431951448004, b = 2.1538870168336004, loss = 4.887872560515324, diff= 0.5418069405413455\n",
      "a = 2.1935719654070485, b = 2.1924871015041525, loss = 5.513778822938285, diff= 0.6259062624229612\n",
      "a = 2.2360251767225257, b = 2.2341537374143847, loss = 6.237248033295764, diff= 0.7234692103574787\n",
      "a = 2.2818510476810836, b = 2.279130544146633, loss = 7.07393387879485, diff= 0.836685845499086\n",
      "a = 2.3313175163895368, b = 2.327680495435915, loss = 8.042039194134563, diff= 0.9681053153397139\n",
      "a = 2.384713807072091, b = 2.3800874567443002, loss = 9.162734153415485, diff= 1.1206949592809217\n",
      "a = 2.4423521211280246, b = 2.4366578449865828, loss = 10.460643314732845, diff= 1.2979091613173601\n",
      "a = 2.504569462534023, b = 2.497722420111463, loss = 11.964412873425054, diff= 1.503769558692209\n",
      "a = 2.5717296082640684, b = 2.5636382190133875, loss = 13.707371354485378, diff= 1.7429584810603238\n",
      "a = 2.6442252352476734, b = 2.634790643082392, loss = 15.728299158736437, diff= 2.020927804251059\n",
      "a = 2.722480216302529, b = 2.7115957115975755, loss = 18.072324922144325, diff= 2.3440257634078883\n",
      "a = 2.8069520984655827, b = 2.794502494139532, loss = 20.791969612687488, diff= 2.719644690543163\n",
      "a = 2.898134778213048, b = 2.8839957362437363, loss = 23.94836274390974, diff= 3.156393131222252\n",
      "a = 2.9965613892110126, b = 2.9805986936467517, loss = 27.612659109561356, diff= 3.664296365651616\n",
      "a = 3.102807419480966, b = 3.0848761916967455, loss = 31.867689133809527, diff= 4.255030024248171\n",
      "a = 3.2174940762059165, b = 3.197437927816297, loss = 36.80988139619171, diff= 4.942192262382186\n",
      "a = 3.3412919178507012, b = 3.318942036326583, loss = 42.55150225774432, diff= 5.7416208615526045\n",
      "a = 3.474924774833034, b = 3.45009893647602, loss = 49.223264933652416, diff= 6.671762675908099\n",
      "a = 3.619173981668966, b = 3.5916754861723055, loss = 56.97736900204899, diff= 7.754104068396572\n",
      "a = 3.7748829453375836, b = 3.7444994657042043, loss = 65.9910414106587, diff= 9.013672408609708\n",
      "a = 3.942962076575589, b = 3.909464417668837, loss = 76.4706617786785, diff= 10.479620368019809\n",
      "a = 4.124394112934434, b = 4.087534871402922, loss = 88.6565684654272, diff= 12.185906686748694\n",
      "a = 4.320239864723243, b = 4.279751982464554, loss = 102.82865781008442, diff= 14.172089344657223\n",
      "a = 4.531644417433342, b = 4.487239620138877, loss = 119.31290751138003, diff= 16.48424970129561\n",
      "a = 4.759843826909176, b = 4.711210938560509, loss = 138.4889767471235, diff= 19.17606923574347\n",
      "a = 5.00617234641145, b = 4.952975469873266, loss = 160.79906083753733, diff= 22.310084090413824\n",
      "a = 5.2720702278282205, b = 5.213946780899992, loss = 186.758207623707, diff= 25.95914678616967\n",
      "a = 5.559092142646631, b = 5.495650738090067, loss = 216.96633695082755, diff= 30.20812932712056\n",
      "a = 5.868916271921664, b = 5.7997344290687325, loss = 252.1222445166576, diff= 35.155907565830034\n",
      "a = 6.203354118389809, b = 6.127975792951376, loss = 293.03991780241705, diff= 40.91767328575946\n",
      "a = 6.564361098097841, b = 6.482294015730044, loss = 340.66754593426515, diff= 47.6276281318481\n",
      "a = 6.954047973474634, b = 6.864760751512677, loss = 396.10966839617373, diff= 55.44212246190858\n",
      "a = 7.374693194693746, b = 7.277612235224254, loss = 460.652981006603, diff= 64.54331261042927\n",
      "a = 7.828756221485161, b = 7.723262357591286, loss = 535.7964032030355, diff= 75.14342219643254\n",
      "a = 8.318891903287158, b = 8.204316778857455, loss = 623.2861104556066, diff= 87.48970725257107\n",
      "a = 8.84796600181724, b = 8.723588163751495, loss = 725.1563518905596, diff= 101.87024143495296\n",
      "a = 9.419071946820646, b = 9.284112626784301, loss = 843.7770086688661, diff= 118.62065677830651\n",
      "a = 10.035548922965207, b = 9.889167484028794, loss = 981.9090065076654, diff= 138.1319978387993\n",
      "a = 10.70100139363433, b = 10.542290415175026, loss = 1142.7688796489492, diff= 160.85987314128386\n",
      "a = 11.419320175771297, b = 11.247300147898612, loss = 1330.1039978785009, diff= 187.33511822955165\n",
      "a = 12.19470518899682, b = 12.008318785481395, loss = 1548.2802178975166, diff= 218.17622001901577\n",
      "a = 13.031690012011104, b = 12.829795908231167, loss = 1802.384011297665, diff= 254.10379340014833\n",
      "a = 13.935168389858594, b = 13.716534589618373, loss = 2098.341460401781, diff= 295.9574491041162\n",
      "a = 14.910422847040076, b = 14.673719479242903, loss = 2443.0569082468865, diff= 344.71544784510525\n",
      "a = 15.963155573769367, b = 15.706947116828566, loss = 2844.574509255238, diff= 401.51760100835145\n",
      "a = 17.09952176596269, b = 16.822258654487303, loss = 3312.26646344266, diff= 467.6919541874222\n",
      "a = 18.326165613895444, b = 18.026175178576192, loss = 3857.0523419121337, diff= 544.7858784694736\n",
      "a = 19.6502591499475, b = 19.325735837669594, loss = 4491.6546395102605, diff= 634.6022975981268\n",
      "a = 21.079544182574963, b = 20.728538999576013, loss = 5230.896538938448, diff= 739.2418994281879\n",
      "a = 22.622377561690993, b = 22.242786678039494, loss = 6092.048859180075, diff= 861.1523202416265\n",
      "a = 24.287780040116726, b = 23.877332488882907, loss = 7095.234312983004, diff= 1003.1854538029293\n",
      "a = 26.085489016789047, b = 25.641733415986714, loss = 8263.898540302076, diff= 1168.6642273190719\n",
      "a = 28.026015470108103, b = 27.54630568977232, loss = 9625.358948493427, diff= 1361.4604081913512\n",
      "a = 30.120705414306677, b = 29.602185104904514, loss = 11211.444212290126, diff= 1586.0852637966982\n",
      "a = 32.38180623816915, b = 31.821392129883012, loss = 13059.239409855856, diff= 1847.7951975657306\n",
      "a = 34.82253831397409, b = 34.216902189210714, loss = 15211.954245239427, diff= 2152.714835383571\n",
      "a = 37.45717229534912, b = 36.802721529069636, loss = 17719.93469028343, diff= 2507.9804450440024\n",
      "a = 40.30111255598867, b = 39.59396911008168, loss = 20641.841737992305, diff= 2921.9070477088753\n",
      "a = 43.37098725709055, b = 42.60696500597078, loss = 24046.024873209903, diff= 3404.1831352175977\n",
      "a = 46.6847455701242, b = 45.859325824982385, loss = 28012.12242686223, diff= 3966.097553652329\n",
      "a = 50.26176262337988, b = 49.3700677119774, loss = 32632.926293798333, diff= 4620.803866936101\n",
      "a = 54.1229527859082, b = 53.1597175334406, loss = 38016.55468586778, diff= 5383.628392069444\n",
      "a = 58.290891951206824, b = 57.25043289548866, loss = 44288.98380632338, diff= 6272.429120455599\n",
      "a = 62.78994953563183, b = 61.666131696607685, loss = 51596.99773790571, diff= 7308.013931582333\n",
      "a = 67.64643096331164, b = 66.43263197259901, loss = 60111.62563193846, diff= 8514.627894032754\n",
      "a = 72.8887314706552, b = 71.57780285138897, loss = 70032.14669885495, diff= 9920.521066916488\n",
      "a = 78.54750212972964, b = 77.13172750031653, loss = 81590.75679909713, diff= 11558.610100242178\n",
      "a = 84.65582906122562, b = 83.12687901863102, loss = 95058.00592875038, diff= 13467.249129653253\n",
      "a = 91.24942688484583, b = 89.59831030362132, loss = 110749.13394952947, diff= 15691.128020779084\n",
      "a = 98.36684753819677, b = 96.58385900049984, loss = 129031.45295069157, diff= 18282.319001162105\n",
      "a = 106.0497056851216, b = 104.12436873435766, loss = 150332.9491438667, diff= 21301.496193175117\n",
      "a = 114.3429220314084, b = 112.26392791770597, loss = 175152.30575480504, diff= 24819.356610938354\n",
      "a = 123.29498597051, b = 121.05012752988183, loss = 204070.5816576352, diff= 28918.27590283015\n",
      "a = 132.95823909493194, b = 130.53433937552177, loss = 237764.8192769151, diff= 33694.23761927991\n",
      "a = 143.3891812309442, b = 140.77201644904557, loss = 277023.90046882397, diff= 39259.08119190886\n",
      "a = 154.64880078596278, b = 151.82301716134387, loss = 322767.02174361097, diff= 45743.121274787\n",
      "a = 166.80293134010122, b = 163.75195532438198, loss = 376065.22154005856, diff= 53298.19979644759\n",
      "a = 179.92263656683775, b = 176.6285779400357, loss = 438166.46374611766, diff= 62101.242206059105\n",
      "a = 194.0846257333807, b = 190.52817300204396, loss = 510524.8649522766, diff= 72358.40120615892\n",
      "a = 209.37170221011107, b = 205.53200969544343, loss = 594834.7499764949, diff= 84309.88502421835\n",
      "a = 225.87324761147994, b = 221.7278135672777, loss = 693070.3332847739, diff= 98235.58330827893\n",
      "a = 243.68574439907226, b = 239.21027944684266, loss = 807531.9556982953, diff= 114461.6224135214\n",
      "a = 262.91334000243114, b = 258.0816251144497, loss = 940899.9593127399, diff= 133368.00361444463\n",
      "a = 283.6684557559862, b = 278.4521889559376, loss = 1096297.462453911, diff= 155397.50314117118\n",
      "a = 306.07244421246367, b = 300.4410750973439, loss = 1277363.504946265, diff= 181066.04249235382\n",
      "a = 330.2562986760072, b = 324.1768497917579, loss = 1488338.2768596555, diff= 210974.7719133906\n",
      "a = 356.3614191035603, b = 349.7982931300401, loss = 1734162.42691341, diff= 245824.15005375445\n",
      "a = 384.5404388526395, b = 377.4552104705683, loss = 2020592.7764844084, diff= 286430.34957099846\n",
      "a = 414.9581171093885, b = 407.3093083323357, loss = 2354337.1494108643, diff= 333744.3729264559\n",
      "a = 447.79230221482806, b = 439.53513987264023, loss = 2743211.475505584, diff= 388874.3260947196\n",
      "a = 483.23497152174866, b = 474.3211254774521, loss = 3196322.8473781105, diff= 453111.3718725266\n",
      "a = 521.4933538621597, b = 511.8706544317247, loss = 3724282.8180330265, diff= 527959.970654916\n",
      "a = 562.7911411882194, b = 552.4032741109762, loss = 4339455.934998995, diff= 615173.1169659682\n",
      "a = 607.3697964709596, b = 596.1559736471953, loss = 5056249.332039338, diff= 716793.3970403429\n",
      "a = 655.4899655039235, b = 643.3845695745042, loss = 5891450.161129614, diff= 835200.8290902767\n"
     ]
    }
   ],
   "source": [
    "def loss(X, Y, a, b):\n",
    "    s = 0\n",
    "    for i in range(X.size):\n",
    "        s += (Y[i] - (a*X[i] + b))**2\n",
    "    return s\n",
    "\n",
    "def update_a(X, Y, a, b):\n",
    "    s = 0\n",
    "    for i in range(X.size):\n",
    "        s += (Y[i] - (a*X[i] + b))*X[i]\n",
    "    return a - eta * s\n",
    "\n",
    "def update_b(X, Y, a, b):\n",
    "    s = 0\n",
    "    for i in range(X.size):\n",
    "        s += (Y[i] - (a*X[i] + b))\n",
    "    return a - eta * s\n",
    "\n",
    "X = np.array([0,1,2])\n",
    "Y = np.array([0.9, 3.1, 5.1])\n",
    "\n",
    "a = 2\n",
    "b = 1\n",
    "eta = 0.01\n",
    "\n",
    "for i in range(100):\n",
    "    a_new = update_a(X, Y, a, b)\n",
    "    b_new = update_b(X, Y, a, b)\n",
    "    loss_new = loss(X, Y, a_new, b_new)\n",
    "    diff = abs(loss(X, Y, a, b) - loss(X, Y, a_new, b_new))\n",
    "    print(\"a = {0}, b = {1}, loss = {2}, diff= {3}\".format(a_new, b_new, loss_new, diff))\n",
    "    a = a_new\n",
    "    b = b_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
