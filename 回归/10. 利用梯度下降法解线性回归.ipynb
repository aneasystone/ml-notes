{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们前面在求解线性回归时，都是通过下面的这个被叫做**正规方程**的式子来计算线性回归的：\n",
    "\n",
    "$$\n",
    "\\bf{a} = (\\rm{X}^T\\rm{X})^{-1}\\rm{X}^T\\bf{y}\n",
    "$$\n",
    "\n",
    "但是使用正规方程求解线性回归有一个限制，$\\rm{X}^T\\rm{X}$ 必须是可逆的，而且它面临的一个最大问题就是当数据量或维度不断增加时，矩阵计算的开销也会越来越大，要知道，计算矩阵的逆时间复杂度是矩阵维度的三次方，因此当 n 过大时，使用正规方程的时间会很长。数据量上百上千对于现在的计算机来说运算速度还是很快的，但 n>10000 时，就应该犹豫是否继续采用正规方程法了。\n",
    "\n",
    "这一节我们将学习一种新的方法，**梯度下降**（Gradient Descent），它比正规方程更具有广泛性，可以用于很多复杂算法的求解。为了理解梯度下降，首先我们得知道什么是**梯度**（Gradient）？但要知道什么是梯度，还得从函数的导数说起。\n",
    "\n",
    "导数可以表示函数在某点的切线斜率，也就是函数在该点附近的变化率，如果导数大于零，那么函数在区间内单调递增，如果导数小于零，函数在区间内则是单调递减。\n",
    "\n",
    "![](../images/gradient-1.jpg)\n",
    "\n",
    "在上面的二次曲线中，我们要求函数在 P0 处的导数，也就是求该点的变化率，变化率表示自变量变化 $\\Delta x$ 趋于无穷小的时候，函数值的变化 $\\Delta y$ 与自变量变化 $\\Delta x$ 的比值： $\\frac{\\Delta y}{\\Delta x}$，可以得到导数的计算公式：\n",
    "\n",
    "$$\n",
    "f'(x) = \\lim\\limits_{\\Delta x \\to 0}\\frac{\\Delta y}{\\Delta x}\n",
    "      = \\lim\\limits_{\\Delta x \\to 0}\\frac{f(x_0 + \\Delta x) - f(x_0)}{\\Delta x}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
