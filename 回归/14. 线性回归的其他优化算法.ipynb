{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降的优化算法\n",
    "\n",
    "前面介绍了梯度下降的三种形式：BGD、SGD 和 MBGD。但在实际运用中还会面临很多挑战，具体的可以参考 Sebastian Ruder 的这篇论文 [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)，这里是相应的 [中文翻译](https://blog.csdn.net/google19890102/article/details/69942970)，论文中提到了 6 种优化算法：\n",
    "\n",
    "* Momentum（动量法）\n",
    "* NAG（Nesterov加速梯度下降法，Nesterov accelerated gradient）\n",
    "* Adagrad\n",
    "* Adadelta\n",
    "* RMSprop\n",
    "* Adam（自适应矩估计，Adaptive Moment Estimation）\n",
    "\n",
    "### 牛顿法\n",
    "\n",
    "#### 牛顿法\n",
    "\n",
    "#### 高斯-牛顿迭代算法\n",
    "\n",
    "高斯-牛顿迭代算法（Gauss-Newton iteration method）是另一种经常用来求解非线性最小二乘的迭代法，其原理是利用了 **泰勒展开公式**，其最大优点是收敛速度快。\n",
    "\n",
    "Taylor 级数求得原目标函数的二阶近似：\n",
    "\n",
    "$$\n",
    "f(x) \\approx \\phi(x) \n",
    "= f(x^{(k)}) + \\nabla f(x^{(k)})(x - x^{(k)}) + \\frac{1}{2}(x - x^{(k)})^T \\nabla^2 f(x^{k})(x - x^{(k)})\n",
    "$$\n",
    "\n",
    "把 $x$ 看作自变量，所有带有 $x^{(k)}$ 的项看作常量，令一阶导数等于 0，即可求近似函数的最小值：\n",
    "\n",
    "$$\n",
    "P_k = -(\\nabla^2 f(x_k))^{-1} \\nabla f(x_k) = - {Hesse}^{-1} {Jacobi}\n",
    "$$\n",
    "\n",
    "上边的 Hesse 矩阵，是一个多元函数的二阶偏导数构成的方阵，描述了函数的局部曲率。\n",
    "\n",
    "本质上来看，牛顿法是二阶收敛，而梯度下降则为一阶收敛，所以牛顿法更快。简单来说，梯度下降是从所处位置选择一个坡度最大的方向走一步，而牛顿法则在选择方向时，不仅考虑坡度，还会考虑下一步的坡度是否会变得更大。\n",
    "\n",
    "几何上来说，牛顿法是用一个二次曲面去拟合当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。\n",
    "\n",
    "对于二元的情况，根据上述泰勒展开公式及求导，取0，可以得到如下迭代公式：\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)}\n",
    "$$\n",
    "\n",
    "可以看出，因为我们需要求矩阵逆，当 Hesse 矩阵不可逆时无法计算。而且矩阵的逆计算复杂度为n的立方，当规模很大时，计算量超大，通常改良做法是采用 **拟牛顿法**，如 BFGS、L-BFGS 等。此外，如果初始值离局部极小值太远，Taylor 展开并不能对原函数进行良好的近似。\n",
    "\n",
    "#### 拟牛顿法\n",
    "\n",
    "### 共轭梯度\n",
    "\n",
    "共轭梯度法（Conjugate gradient, CG）\n",
    "\n",
    "https://cosx.org/2016/11/conjugate-gradient-for-regression/\n",
    "\n",
    "### 近端梯度\n",
    "\n",
    "近端梯度法（Proximal Gradient Method ，PG）\n",
    "\n",
    "https://blog.csdn.net/qq547276542/article/details/78251779\n",
    "\n",
    "http://roachsinai.github.io/2016/08/03/1Proximal_Method/\n",
    "\n",
    "### 参考\n",
    "\n",
    "* [梯度下降优化算法综述](https://blog.csdn.net/google19890102/article/details/69942970)\n",
    "* [一文通透优化算法：从随机梯度、随机梯度下降法到牛顿法、共轭梯度](https://blog.csdn.net/v_JULY_v/article/details/81350035)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
