{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们前面在求解线性回归时，都是通过下面的这个被叫做**正规方程**的式子来计算线性回归的：\n",
    "\n",
    "$$\n",
    "\\bf{a} = (\\rm{X}^T\\rm{X})^{-1}\\rm{X}^T\\bf{y}\n",
    "$$\n",
    "\n",
    "但是使用正规方程求解线性回归有一个限制，$\\rm{X}^T\\rm{X}$ 必须是可逆的，而且它面临的一个最大问题就是当数据量或维度不断增加时，矩阵计算的开销也会越来越大，要知道，计算矩阵的逆时间复杂度是矩阵维度的三次方，因此当 n 过大时，使用正规方程的时间会很长。数据量上百上千对于现在的计算机来说运算速度还是很快的，但 n>10000 时，就应该犹豫是否继续采用正规方程法了。\n",
    "\n",
    "我们以一元线性回归为例，在前面的最小二乘法求解过程中我们知道，求解一元线性回归可以转化为求解损失函数的最小值，损失函数是一个关于参数 a, b 的二元二次函数：\n",
    "\n",
    "$$\n",
    "loss = n\\bar{y^2} + a^2n\\bar{x^2} + nb^2 + 2abn\\bar{x} - 2an\\bar{xy} - 2bn\\bar{y}\n",
    "$$\n",
    "\n",
    "它是三维空间里的一个曲面，图像类似于下面左图这样：\n",
    "\n",
    "![](../images/loss-2.jpg)\n",
    "\n",
    "右图代表的是损失函数的**等高图**，每一条线表示损失函数的值相等，红色的叉号表示损失函数的最小值。要求这个最小值，除了前面介绍的正规方程法，这一节我们将学习一种新的方法，**梯度下降**（Gradient Descent），它比正规方程更具有广泛性，可以用于很多复杂算法的求解。\n",
    "\n",
    "梯度下降算法的基本思路是，在坡上任意取一点，然后沿着下坡方向走最后到达最低点，为了走的最快，我们每次都沿着最陡的方向下坡，这样不断的迭代，直到损失函数收敛到最小值。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用梯度下降算法求解线性回归\n",
    "\n",
    "回到线性回归的一般形式，损失函数可以写成下面这样：\n",
    "\n",
    "$$\n",
    "loss = J(\\theta) = (\\bf{y}-\\rm{X}\\theta)^T (\\bf{y}-\\rm{X}\\theta)\n",
    "$$\n",
    "\n",
    "根据前面的矩阵求导，得到梯度下降的更新公式为：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta &:= \\theta - \\alpha\\frac{\\partial}{\\partial\\theta}J(\\theta) \\\\\n",
    "       &= \\theta - \\alpha X^T(\\bf{y}-\\rm{X}\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，$\\alpha$ 被称为学习率，$:=$ 表示用右边的值替代左边的值，用梯度下降算法的流程就是，设定 $\\theta$ 的初始值，使用梯度下降的更新公式不断的更新 $\\theta$ 的值，直到 $J(\\theta)$ 收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/24913912"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是梯度？\n",
    "\n",
    "为了更好的理解梯度下降，我们来看看什么是**梯度**（Gradient），而要知道什么是梯度，还得从函数的导数说起。\n",
    "\n",
    "导数的公式如下所示：\n",
    "\n",
    "$$\n",
    "f'(x) = \\lim\\limits_{\\Delta x \\to 0}\\frac{\\Delta y}{\\Delta x}\n",
    "      = \\lim\\limits_{\\Delta x \\to 0}\\frac{f(x_0 + \\Delta x) - f(x_0)}{\\Delta x}\n",
    "$$\n",
    "\n",
    "它可以表示函数在某点的切线斜率，也就是函数在该点附近的变化率，如果导数大于零，那么函数在区间内单调递增，如果导数小于零，函数在区间内则是单调递减。\n",
    "\n",
    "![](../images/gradient-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标准化和归一化\n",
    "\n",
    "在实际的运用中我们选取的特征，比如长度，重量，面积等等，通常单位和范围都不同，这会导致梯度下降算法变慢，所以我们要将特征缩放到相对统一的范围内。通常的方法有 **标准化（Standardization）** 和 **归一化（Normalization）**。\n",
    "\n",
    "标准化是把数据变成符合标准的正态分布，由 **中心极限定理** 可知，当数据量足够大时，无论原来的数据是何种分布，都可以通过下面的更新公式转变成正态分布：\n",
    "\n",
    "$$\n",
    "x_i := \\frac{x_i-\\mu}{\\delta}\n",
    "$$\n",
    "\n",
    "归一化对梯度下降算法很友好，可以让算法最终收敛并且提高训练速度和精度，归一化的更新公式为：\n",
    "\n",
    "$$\n",
    "x_i := \\frac{x_i-min(x_i)}{max(x_i)-min(x_i)}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
