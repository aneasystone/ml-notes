{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面在使用 sklearn 解二元线性回归时发现画出来的图像和之前计算出来的不太一样，这其实是因为计算的方法不一样。我们是通过下面的这个被叫做**正规方程**的式子来计算线性回归的：\n",
    "\n",
    "$$\n",
    "\\bf{a} = (\\rm{X}^T\\rm{X})^{-1}\\rm{X}^T\\bf{y}\n",
    "$$\n",
    "\n",
    "但是使用正规方程求解线性回归有一个限制，$\\rm{X}^T\\rm{X}$ 必须是可逆的，而且它面临的一个最大问题就是当数据量或维度不断增加时，矩阵计算的开销也会越来越大，要知道，计算矩阵的逆时间复杂度是矩阵维度的三次方，因此当 n 过大时，使用正规方程的时间会很长。数据量上百上千对于现在的计算机来说运算速度还是很快的，但 n>10000 时，就应该犹豫是否继续采用正规方程法了。\n",
    "\n",
    "这一节我们将学习一种新的方法，**梯度下降**（Gradient Descent），它比正规方程更具有广泛性，可以用于很多复杂算法的求解。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
