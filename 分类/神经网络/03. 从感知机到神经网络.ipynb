{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过前面的学习，我们掌握了感知机模型和神经元模型，我们稍微对比一下。\n",
    "\n",
    "感知机模型如下：\n",
    "\n",
    "$$\n",
    "y = sign(w^Tx + b)\n",
    "$$\n",
    "\n",
    "其中 $sign(z)$ 为符号函数，也就是阶跃函数。\n",
    "\n",
    "神经元模型如下：\n",
    "\n",
    "$$\n",
    "y = f(\\sum_{i=1}^n w_i x_i - \\theta)\n",
    "$$\n",
    "\n",
    "其中 $f(z)$ 可以是 $sign(z)$ 或 $sigmoid(z)$ 或其他类型的激活函数。\n",
    "\n",
    "可以看出两个模型实际上是一样的，可以说感知机就是一个简化版神经元，如下所示：\n",
    "\n",
    "<img src=\"../../images/neutron-1.jpg\" width=\"300px\" />\n",
    "\n",
    "这实际上是一个神经网络，左边的输入节点被称为 **输入层**，右边的输出节点被称为 **输出层**。要注意的是，一般我们根据网络中计算节点的层数来命名神经网络，在上面的模型中，左边的输入层只负责传输数据，并没有进行计算，只有右边的输出层进行了计算，所以被称为 **单层神经网络**。有计算过程的神经元叫做 **功能神经元**（functional neuron）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用一个例子来展示下具体的神经元模型，譬如我们要解决下面的三种分类问题（AND、OR、NOT）：\n",
    "\n",
    "<div>\n",
    "    <img src=\"../../images/and.png\" style=\"display:inline\" width=\"300px\" />\n",
    "    <img src=\"../../images/or.png\" style=\"display:inline\" width=\"300px\" />\n",
    "    <img src=\"../../images/not.png\" style=\"display:inline\" width=\"300px\" />\n",
    "</div>\n",
    "\n",
    "很显然这三个问题都是线性可分的，我们可以构造出如下的神经元模型：\n",
    "\n",
    "<img src=\"../../images/neutron-and-or-not.png\" width=\"800px\" />\n",
    "\n",
    "但是如果我们要处理下面的异或问题，这是一个线性不可分问题，单层的神经网络就无法解决了。\n",
    "\n",
    "<img src=\"../../images/xor.png\" width=\"300px\" />\n",
    "\n",
    "我们在神经网络中增加一层，构造出下面这样的神经网络：\n",
    "\n",
    "<img src=\"../../images/xor-neotron.png\" width=\"500px\" />\n",
    "\n",
    "这是一个经典的两层神经网络，除了最左边的输入层和最右边的输出层，我们在中间增加了一层，被称为 **隐藏层**（hidden layer），这样的两层神经网络有时候也被称为 **单隐层神经网络**。可以看出，在这个网络里每一层神经元与下一层神经元全互联，神经元之间不存在同层连接，也不存在跨层连接，这样的神经网络通常称为 **多层前馈神经网络**（multi-layer feedforward neural networks），所谓的前馈指的是，数据从输入节点到输出节点向前传递，没有输出的信息传递到输入层，即没有反馈，表现在图形上是有向图没有回路。当然也有 **反馈神经网络**（recurrent neural network），比如 Hopfield 网络、布尔兹曼机等。神经网络的学习过程，就是根据训练数据来调整神经元之间的连接权重和每个神经元的阈值。\n",
    "\n",
    "两层神经网络比单层神经网络具有更强的表现力，理论上可以证明，两层神经网络可以无限逼近任意连续函数，也就是说它可以解决非线性分类问题。为什么单层神经网络只能处理线性问题，而增加一层就可以处理非线性问题呢？这是因为增加的一层神经元，相当于对数据作了空间变换。也就是说，隐藏层对原始的数据进行了一个空间变换，使其可以被线性分类，然后输出层的决策分界划出了一个线性分类分界线，对其进行分类。\n",
    "\n",
    "在多层神经网络的基础上，后来又继续发展成了 **深度学习**（deep learning），其在语音识别、图像识别等领域得到了广泛的应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考\n",
    "\n",
    "1. https://www.cnblogs.com/subconscious/p/5058741.html\n",
    "1. https://www.zhihu.com/question/22553761/answer/36429105\n",
    "1. http://fitzeng.org/2018/02/19/MLNeuralNetwork/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
