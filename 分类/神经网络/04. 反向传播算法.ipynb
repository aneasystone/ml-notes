{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一节中，我们构造了三个两层神经网络来解决 AND、OR、NOT 问题，并构造了一个三层神经网络来解决 XOR 问题，那么这些神经网络结构是如何构造出来的呢？神经元之间的权重和每个神经元的阈值又是如何确定的呢？如果是线性可分问题，两层神经网络就可以解决，这也就是感知机模型，通过前面学习的随机梯度下降法来训练感知机即可求解，如果是线性不可分问题，需要构造更复杂的多层网络结构，通常使用 **反向传播算法**（error BackPropagation，简称 **BP 算法**，也叫做 **误差逆传播算法**）。\n",
    "\n",
    "### 神经网络的符号表示\n",
    "\n",
    "假设我们要求解的神经网络如下图所示：\n",
    "\n",
    "<img src=\"../../images/bp-method.png\" width=\"800px\" />\n",
    "\n",
    "该神经网络的特点如下：\n",
    "\n",
    "* 输入层有 $d$ 个节点，表示输入的特征向量为 $d$ 维\n",
    "* 输出层有 $l$ 个节点，表示输出向量为 $l$ 维，也就是 $l$ 类分类问题，$l = 2$ 时就是二分类问题\n",
    "* 隐层有 $q$ 个节点\n",
    "* 第 $i$ 个输入层神经元和第 $h$ 个隐层神经元之间的连接权重为 $v_{ih}$\n",
    "* 第 $h$ 个隐层神经元的阈值为 $\\gamma_h$\n",
    "* 第 $h$ 个隐层神经元和第 $j$ 个输出层神经元之间的连接权重为 $w_{hj}$\n",
    "* 第 $j$ 个输出层神经元的阈值为 $\\theta_j$\n",
    "\n",
    "所以有，第 $h$ 个隐层神经元接受到的输入为：\n",
    "\n",
    "$$\n",
    "\\alpha_h = v_{1h}x_1 + v_{2h}x_2 + \\dots + v_{dh}x_d = \\sum_{i=1}^d v_{ih}x_i\n",
    "$$\n",
    "\n",
    "它的输出为：\n",
    "\n",
    "$$\n",
    "b_h = f(\\alpha_h - \\gamma_h)\n",
    "$$\n",
    "\n",
    "这里的 $f(z)$ 表示激活函数，譬如 Sigmoid 函数。最后得到，第 $j$ 个输出层神经元的输入为：\n",
    "\n",
    "$$\n",
    "\\beta_j = w_{1j}b_1 + w_{2j}b_2 + \\dots + w_{qj}b_q = \\sum_{h=1}^q w_{hj}b_h\n",
    "$$\n",
    "\n",
    "它的输出为：\n",
    "\n",
    "$$\n",
    "y_j = f(\\beta_j - \\theta_j)\n",
    "$$\n",
    "\n",
    "### 神经网络的损失函数\n",
    "\n",
    "从上面的计算过程中可以看出，这里一共有 $dq + lq + q + l$ 个参数：输入层到隐层的 $dq$ 个权值，隐层到输出层的 $lq$ 个权值，$q$ 个隐层神经元的阈值，$l$ 个输出层神经元的阈值。要求解这些参数，一个很容易想到的方法是使用梯度下降法，首先定义神经网络的损失函数，然后给每个参数一个初始值，再根据损失函数的梯度对初始值迭代更新，最终收敛。那么神经网络的损失函数该如何定义呢？\n",
    "\n",
    "假设对于输入样本 $\\bf{x}_k$ 我们有输出 $\\hat{\\bf{y}}_k = (\\hat{y}_1^k, \\hat{y}_2^k, \\dots, \\hat{y}_l^k)$，和线性回归一样，我们可以得到预测值和真实值的平方误差：\n",
    "\n",
    "$$\n",
    "E_k = \\frac{1}{2} \\sum_{j=1}^l (\\hat{y}_j^k - y_j^k)^2\n",
    "$$\n",
    "\n",
    "很显然，我们可以把这个函数当作神经网络的损失函数，我们的目标就是让它的值最小。不过要注意的是，这里的损失函数是定义在某一个样本上的，也就是说每次仅针对一个训练样本更新连接权值和阈值，这种方法叫做 **标准BP算法**。如果我们把损失函数定义成所有样本损失的平均值的话：\n",
    "\n",
    "$$\n",
    "E = \\frac{1}{m} \\sum_{k=1}^m E_k\n",
    "$$\n",
    "\n",
    "这就是 **累积BP算法**。这有点类似于随机梯度下降和标准梯度下降。\n",
    "\n",
    "### BP算法的推导\n",
    "\n",
    "### 过拟合"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
