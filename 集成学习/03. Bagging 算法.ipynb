{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "集成学习中个体学习器应尽可能的相互独立，要设法使基学习器尽可能具有较大差异。一种最简单的做法是对训练样本进行采样，产生若干个不同的子集，然后基于每一个子集训练出一个基学习器，这样得到的基学习器由于训练样本不同，他们之间有希望能得到比较大的差异。但是如果要保证每个子集样本都不同，那么每个子集的训练数据也会比较少，这样得到的基学习器性能就会比较差，所以常常使用交叉采样法。\n",
    "\n",
    "**Bagging**（Bootstrap AGGregatING） 是并形式集成学习方法最著名的代表，它基于 **自助采样法**（bootstrap sampling），它的基本流程如下：首先从数据集中随机取出一个样本放入采样集，再把该样本放回，这样经过 m 次随机采样，就得到了一个包含 m 个样本的采样集，使用同样的方法重复 T 次，我们就得到了 T 个采样集，然后基于每个采样集训练出一个基学习器，最后将这些基学习器进行结合。\n",
    "\n",
    "标准的 AdaBoost 只适用于二分类任务，而 Bagging 可以直接用于多分类和回归任务。另外，由于 Bagging 算法采用了自助采样法，所以每个基学习器只使用了训练集的约 63.2% 的样本，剩下约 36.8% 的样本称为 **包外样本**，可用于验证集对泛化性能进行 **包外估计**（out-of-bag estimate），还可以根据不同的基学习器采用不同的方法辅助学习器的训练：比如基学习器是决策树时，可用来辅助剪枝，基学习器是神经网络时，可用来辅助早停减少过拟合风险。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
