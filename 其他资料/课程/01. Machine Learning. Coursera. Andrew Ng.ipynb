{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 课程名称：Machine Learning\n",
    "* 课程来源：https://www.coursera.org/\n",
    "* 课程地址：https://www.coursera.org/learn/machine-learning\n",
    "* 讲师：Andrew Ng\n",
    "\n",
    "## Week 1\n",
    "\n",
    "### Introduction\n",
    "\n",
    "#### Welcome to Machine Learning!\n",
    "#### Welcome\n",
    "#### What is Machine Learning?\n",
    "#### Supervised Learning\n",
    "#### Unsupervised Learning\n",
    "\n",
    "### Linear Regression with One Variable\n",
    "\n",
    "#### Model Representation\n",
    "#### Cost Function\n",
    "#### Cost Function - Intuition I\n",
    "#### Cost Function - Intuition II\n",
    "#### Gradient Descent\n",
    "#### Gradient Descent Intuition\n",
    "#### Gradient Descent For Linear Regression\n",
    "\n",
    "### Linear Algebra Review\n",
    "\n",
    "#### Matrices and Vectors\n",
    "#### Addition and Scalar Multiplication\n",
    "#### Matrix Vector Multiplication\n",
    "#### Matrix Matrix Multiplication\n",
    "#### Matrix Multiplication Properties\n",
    "#### Inverse and Transpose\n",
    "\n",
    "## Week 2\n",
    "\n",
    "### Linear Regression with Multiple Variables\n",
    "\n",
    "#### Multiple Features\n",
    "#### Gradient Descent for Multiple Variables\n",
    "#### Gradient Descent in Practice I - Feature Scaling\n",
    "#### Gradient Descent in Practice II - Learning Rate\n",
    "#### Features and Polynomial Regression\n",
    "#### Normal Equation\n",
    "#### Normal Equation Noninvertibility\n",
    "#### Working on and Submitting Programming Assignments\n",
    "\n",
    "### Octave/Matlab Tutorial\n",
    "\n",
    "#### Basic Operations\n",
    "#### Moving Data Around\n",
    "#### Computing on Data\n",
    "#### Plotting Data\n",
    "#### Control Statements: for, while, if statement\n",
    "#### Vectorization\n",
    "\n",
    "## Week 3\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "#### Classification\n",
    "#### Hypothesis Representation\n",
    "#### Decision Boundary\n",
    "#### Cost Function\n",
    "#### Simplified Cost Function and Gradient Descent\n",
    "#### Advanced Optimization\n",
    "#### Multiclass Classification: One-vs-all\n",
    "\n",
    "### Regularization\n",
    "\n",
    "#### The Problem of Overfitting\n",
    "#### Cost Function\n",
    "#### Regularized Linear Regression\n",
    "#### Regularized Logistic Regression\n",
    "\n",
    "## Week 4\n",
    "\n",
    "### Neural Networks: Representation\n",
    "\n",
    "#### Non-linear Hypotheses\n",
    "#### Neurons and the Brain\n",
    "#### Model Representation I\n",
    "#### Model Representation II\n",
    "#### Examples and Intuitions I\n",
    "#### Examples and Intuitions II\n",
    "#### Multiclass Classification\n",
    "\n",
    "## Week 5\n",
    "\n",
    "### Neural Networks: Learning\n",
    "\n",
    "#### Cost Function\n",
    "#### Backpropagation Algorithm\n",
    "#### Backpropagation Intuition\n",
    "#### Implementation Note: Unrolling Parameters\n",
    "#### Gradient Checking\n",
    "#### Random Initialization\n",
    "#### Putting It Together\n",
    "#### Autonomous Driving\n",
    "\n",
    "## Week 6\n",
    "\n",
    "### Advice for Applying Machine Learning\n",
    "\n",
    "#### Deciding What to Try Next\n",
    "#### Evaluating a Hypothesis\n",
    "#### Model Selection and Train/Validation/Test Sets\n",
    "#### Diagnosing Bias vs. Variance\n",
    "#### Regularization and Bias/Variance\n",
    "#### Learning Curves\n",
    "#### Deciding What to Do Next Revisited\n",
    "\n",
    "### Machine Learning System Design\n",
    "\n",
    "#### Prioritizing What to Work On\n",
    "#### Error Analysis\n",
    "#### Error Metrics for Skewed Classes\n",
    "#### Trading Off Precision and Recall\n",
    "#### Data For Machine Learning\n",
    "\n",
    "## Week 7\n",
    "\n",
    "### Support Vector Machines\n",
    "\n",
    "#### Optimization Objective\n",
    "#### Large Margin Intuition\n",
    "#### Mathematics Behind Large Margin Classification\n",
    "#### Kernels I\n",
    "#### Kernels II\n",
    "#### Using An SVM\n",
    "\n",
    "## Week 8\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "#### Unsupervised Learning: Introduction\n",
    "#### K-Means Algorithm\n",
    "#### Optimization Objective\n",
    "#### Random Initialization\n",
    "#### Choosing the Number of Clusters\n",
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "#### Motivation I: Data Compression\n",
    "#### Motivation II: Visualization\n",
    "#### Principal Component Analysis Problem Formulation\n",
    "#### Principal Component Analysis Algorithm\n",
    "#### Reconstruction from Compressed Representation\n",
    "#### Choosing the Number of Principal Components\n",
    "#### Advice for Applying PCA\n",
    "\n",
    "## Week 9\n",
    "\n",
    "### Anomaly Detection\n",
    "\n",
    "#### Problem Motivation\n",
    "#### Gaussian Distribution\n",
    "#### Algorithm\n",
    "#### Developing and Evaluating an Anomaly Detection System\n",
    "#### Anomaly Detection vs. Supervised Learning\n",
    "#### Choosing What Features to Use\n",
    "#### Multivariate Gaussian Distribution\n",
    "#### Anomaly Detection using the Multivariate Gaussian Distribution\n",
    "\n",
    "### Recommender Systems\n",
    "\n",
    "#### Problem Formulation\n",
    "#### Content Based Recommendations\n",
    "#### Collaborative Filtering\n",
    "#### Collaborative Filtering Algorithm\n",
    "#### Vectorization: Low Rank Matrix Factorization\n",
    "#### Implementational Detail: Mean Normalization\n",
    "\n",
    "## Week 10\n",
    "\n",
    "### Large Scale Machine Learning\n",
    "\n",
    "#### Learning With Large Datasets\n",
    "#### Stochastic Gradient Descent\n",
    "#### Mini-Batch Gradient Descent\n",
    "#### Stochastic Gradient Descent Convergence\n",
    "#### Online Learning\n",
    "#### Map Reduce and Data Parallelism\n",
    "\n",
    "## Week 11\n",
    "\n",
    "### Application Example: Photo OCR\n",
    "\n",
    "#### Problem Description and Pipeline\n",
    "#### Sliding Windows\n",
    "#### Getting Lots of Data and Artificial Data\n",
    "#### Ceiling Analysis: What Part of the Pipeline to Work on Next\n",
    "#### Summary and Thank You"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
